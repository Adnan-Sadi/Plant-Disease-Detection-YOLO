{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Setup\n",
    "\n",
    "Clone repo, install dependencies and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "f3eebb18-60b0-40ec-c979-106c0f2f20c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v6.1-168-g5743deb torch 1.11.0 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete  (8 CPUs, 15.9 GB RAM, 83.6/331.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "!pip install -r requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "from yolov5 import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77j3isVHRYfc"
   },
   "outputs": [],
   "source": [
    "!unzip -q ../dataset.zip -d ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vvfpezb7OUYz"
   },
   "outputs": [],
   "source": [
    "!unzip -q ../Test.zip -d ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLmvDSXnrWc5"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/yolov5/runs/train/exp.zip /content/yolov5/runs/train/exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDpNSTS0nFyc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "dir = '/content/dataset/images/val/ts'\n",
    "for f in os.listdir(dir):\n",
    "  try:\n",
    "      os.remove(os.path.join(dir, f))\n",
    "  except:\n",
    "    continue\n",
    "os.remove(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 1. Inference\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "```shell\n",
    "python detect.py --source 0  # webcam\n",
    "                          img.jpg  # image \n",
    "                          vid.mp4  # video\n",
    "                          path/  # directory\n",
    "                          path/*.jpg  # glob\n",
    "                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR9ZbuQCH7FX",
    "outputId": "829b5f3e-dd6e-42be-a1bc-6ebaf3c0bc5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=potato_lb_4.jpg, data=data/coco128.yaml, imgsz=[256, 256], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.1-39-g4effd06 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7039792 parameters, 0 gradients, 15.9 GFLOPs\n",
      "image 1/1 /content/yolov5/potato_lb_4.jpg: 192x256 1 Tomato_Late_Blight, Done. (0.023s)\n",
      "Speed: 0.4ms pre-process, 22.5ms inference, 3.3ms NMS per image at shape (1, 3, 256, 256)\n",
      "Results saved to \u001b[1mruns/detect/exp20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights best.pt --img 256 --conf 0.25 --source potato_lb_4.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQVvwi56QI0E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Images\n",
    "img1 = Image.open('zidane.jpg')\n",
    "img2 = Image.open('bus.jpg')\n",
    "imgs = [img1, img2]  # batched list of images\n",
    "\n",
    "# Inference\n",
    "result = model(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eq1SMWl6Sfn"
   },
   "source": [
    "# 2. Validate\n",
    "Validate a model's accuracy on [COCO](https://cocodataset.org/#home) val or test-dev datasets. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag. Note that `pycocotools` metrics may be ~1% better than the equivalent repo metrics, as is visible below, due to slight differences in mAP computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyTZYGgRjnMc"
   },
   "source": [
    "## COCO val\n",
    "Download [COCO val 2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L14) dataset (1GB - 5000 images), and test model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48,
     "referenced_widgets": [
      "eb95db7cae194218b3fcefb439b6352f",
      "769ecde6f2e64bacb596ce972f8d3d2d",
      "384a001876054c93b0af45cd1e960bfe",
      "dded0aeae74440f7ba2ffa0beb8dd612",
      "5296d28be75740b2892ae421bbec3657",
      "9f09facb2a6c4a7096810d327c8b551c",
      "25621cff5d16448cb7260e839fd0f543",
      "0ce7164fc0c74bb9a2b5c7037375a727",
      "c4c4593c10904cb5b8a5724d60c7e181",
      "473371611126476c88d5d42ec7031ed6",
      "65efdfd0d26c46e79c8c5ff3b77126cc"
     ]
    },
    "id": "WQPtK1QYVaD_",
    "outputId": "bcf9a448-1f9b-4a41-ad49-12f181faf05a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb95db7cae194218b3fcefb439b6352f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/780M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download COCO val\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../datasets && rm tmp.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X58w8JLpMnjH"
   },
   "outputs": [],
   "source": [
    "# Run YOLOv5x on COCO val\n",
    "!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rc_KbFk0juX2"
   },
   "source": [
    "## COCO test\n",
    "Download [COCO test2017](https://github.com/ultralytics/yolov5/blob/74b34872fdf41941cddcf243951cdb090fbac17b/data/coco.yaml#L15) dataset (7GB - 40,000 images), to test model accuracy on test-dev set (**20,000 images, no labels**). Results are saved to a `*.json` file which should be **zipped** and submitted to the evaluation server at https://competitions.codalab.org/competitions/20794."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0AJnSeCIHyJ"
   },
   "outputs": [],
   "source": [
    "# Download COCO test-dev2017\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017labels.zip', 'tmp.zip')\n",
    "!unzip -q tmp.zip -d ../datasets && rm tmp.zip\n",
    "!f=\"test2017.zip\" && curl http://images.cocodataset.org/zips/$f -o $f && unzip -q $f -d ../datasets/coco/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29GJXAP_lPrt"
   },
   "outputs": [],
   "source": [
    "# Run YOLOv5x on COCO test\n",
    "!python val.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 --half --task test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY2VXXXu74w5"
   },
   "source": [
    "# 3. Train\n",
    "\n",
    "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png\"/></a></p>\n",
    "Close the active learning loop by sampling images from your inference conditions with the `roboflow` pip package\n",
    "<br><br>\n",
    "\n",
    "Train a YOLOv5s model on the [COCO128](https://www.kaggle.com/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n",
    "\n",
    "- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n",
    "automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n",
    "- **[Datasets](https://github.com/ultralytics/yolov5/tree/master/data)** available for autodownload include: [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/ultralytics/yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/ultralytics/yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/ultralytics/yolov5/blob/master/data/SKU-110K.yaml).\n",
    "- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOy5KI2ncnWd"
   },
   "outputs": [],
   "source": [
    "# Tensorboard  (optional)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fLAV42oNb7M"
   },
   "outputs": [],
   "source": [
    "# Weights & Biases  (optional)\n",
    "%pip install -q wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NcFxRcFdJ_O",
    "outputId": "6d925961-fcef-4302-f103-461e1f1c1751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=256, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "remote: Enumerating objects: 4, done.\u001b[K\n",
      "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (4/4), done.\n",
      "From https://github.com/ultralytics/yolov5\n",
      " * [new branch]      glenn-jocher-patch-1 -> origin/glenn-jocher-patch-1\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "YOLOv5 ðŸš€ v6.1-54-ga2d617e torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n",
      "100% 14.1M/14.1M [00:00<00:00, 105MB/s] \n",
      "\n",
      "Overriding model.yaml nc=80 with nc=11\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     43152  models.yolo.Detect                      [11, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 270 layers, 7049296 parameters, 7049296 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/../dataset/labels/train' images and labels...2527 found, 0 missing, 3 empty, 0 corrupt: 100% 2527/2527 [00:01<00:00, 1581.80it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/../dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB ram): 100% 2527/2527 [00:03<00:00, 765.67it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/../dataset/labels/val' images and labels...631 found, 0 missing, 1 empty, 0 corrupt: 100% 631/631 [00:00<00:00, 1195.01it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/../dataset/labels/val.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 631/631 [00:00<00:00, 756.65it/s]\n",
      "Plotting labels to runs/train/exp/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.06 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Image sizes 256 train, 256 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/49    0.577G   0.07506   0.02244   0.06621        39       256: 100% 158/158 [00:49<00:00,  3.19it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.72it/s]\n",
      "                 all        631        715      0.121      0.537      0.175      0.112\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/49    0.667G   0.04745   0.01825   0.05743        41       256: 100% 158/158 [00:45<00:00,  3.46it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.94it/s]\n",
      "                 all        631        715      0.214      0.575      0.389       0.26\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/49    0.667G   0.04377   0.01627   0.04621        42       256: 100% 158/158 [00:45<00:00,  3.51it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.80it/s]\n",
      "                 all        631        715      0.312      0.706       0.54      0.366\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/49    0.667G   0.03837   0.01538   0.03943        47       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.96it/s]\n",
      "                 all        631        715      0.652      0.606      0.641      0.468\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/49    0.667G   0.03531   0.01517   0.03479        42       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.99it/s]\n",
      "                 all        631        715      0.674      0.705      0.726      0.541\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/49    0.667G   0.03288   0.01503   0.03298        45       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.96it/s]\n",
      "                 all        631        715      0.695      0.725      0.777      0.605\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/49    0.667G   0.03165   0.01425   0.03007        37       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.92it/s]\n",
      "                 all        631        715      0.726      0.775      0.796       0.61\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/49    0.667G   0.03128   0.01445   0.02841        46       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.03it/s]\n",
      "                 all        631        715      0.623      0.746      0.749      0.562\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/49    0.667G    0.0291   0.01404   0.02627        43       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.95it/s]\n",
      "                 all        631        715      0.786      0.837      0.865      0.685\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/49    0.667G   0.02928   0.01419   0.02558        47       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.94it/s]\n",
      "                 all        631        715      0.793      0.746      0.833      0.656\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/49    0.667G   0.02854   0.01382   0.02419        42       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.92it/s]\n",
      "                 all        631        715      0.866      0.816      0.874      0.696\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/49    0.667G   0.02786   0.01371   0.02388        38       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.01it/s]\n",
      "                 all        631        715      0.841      0.802      0.873        0.7\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/49    0.667G   0.02757   0.01368    0.0227        49       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.05it/s]\n",
      "                 all        631        715      0.887      0.795      0.876      0.721\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/49    0.667G   0.02729    0.0132   0.02157        48       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.00it/s]\n",
      "                 all        631        715      0.916      0.784      0.884      0.706\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/49    0.667G   0.02614   0.01304   0.02144        38       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.02it/s]\n",
      "                 all        631        715      0.842      0.846      0.878      0.724\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/49    0.667G   0.02621   0.01304   0.02098        43       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.02it/s]\n",
      "                 all        631        715      0.879      0.794      0.866      0.709\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/49    0.667G   0.02622   0.01332    0.0211        38       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.02it/s]\n",
      "                 all        631        715      0.858      0.825      0.898      0.742\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/49    0.667G   0.02497   0.01287   0.02066        47       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.01it/s]\n",
      "                 all        631        715      0.876      0.797      0.885       0.72\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/49    0.667G    0.0247   0.01259   0.01957        48       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.01it/s]\n",
      "                 all        631        715       0.82      0.836       0.88      0.719\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/49    0.667G   0.02453   0.01277   0.01883        44       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.08it/s]\n",
      "                 all        631        715      0.869      0.867      0.911      0.757\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/49    0.667G   0.02422   0.01265   0.01836        49       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.01it/s]\n",
      "                 all        631        715      0.876      0.827       0.89      0.731\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     21/49    0.667G    0.0238   0.01208    0.0178        45       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.04it/s]\n",
      "                 all        631        715      0.904       0.83        0.9      0.758\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     22/49    0.667G   0.02355   0.01236   0.01712        42       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.99it/s]\n",
      "                 all        631        715      0.877      0.788      0.879      0.732\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     23/49    0.667G   0.02371   0.01236   0.01721        46       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.08it/s]\n",
      "                 all        631        715      0.902      0.843      0.912      0.735\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     24/49    0.667G   0.02399   0.01245   0.01632        41       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.07it/s]\n",
      "                 all        631        715      0.853      0.828      0.899      0.752\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     25/49    0.667G   0.02332   0.01236   0.01581        45       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.04it/s]\n",
      "                 all        631        715       0.88      0.877       0.92      0.777\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     26/49    0.667G   0.02273   0.01181   0.01578        44       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.02it/s]\n",
      "                 all        631        715      0.902      0.844       0.91      0.756\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     27/49    0.667G   0.02282   0.01204   0.01633        39       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.04it/s]\n",
      "                 all        631        715      0.902      0.835      0.908      0.755\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     28/49    0.667G   0.02259   0.01202   0.01546        44       256: 100% 158/158 [00:45<00:00,  3.51it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.03it/s]\n",
      "                 all        631        715       0.91      0.844      0.916       0.77\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     29/49    0.667G   0.02196    0.0121   0.01387        56       256: 100% 158/158 [00:45<00:00,  3.50it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.01it/s]\n",
      "                 all        631        715      0.932      0.836      0.928      0.775\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     30/49    0.667G    0.0215   0.01187   0.01422        40       256: 100% 158/158 [00:44<00:00,  3.51it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.03it/s]\n",
      "                 all        631        715      0.901      0.848      0.912      0.778\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     31/49    0.667G   0.02147   0.01181   0.01401        46       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.04it/s]\n",
      "                 all        631        715      0.907       0.87      0.921      0.771\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     32/49    0.667G   0.02178   0.01185   0.01387        34       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.07it/s]\n",
      "                 all        631        715      0.916      0.856      0.933      0.781\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     33/49    0.667G     0.021   0.01141   0.01252        36       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.09it/s]\n",
      "                 all        631        715      0.906      0.844      0.926      0.787\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     34/49    0.667G   0.02097   0.01169   0.01265        41       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.10it/s]\n",
      "                 all        631        715      0.918      0.852      0.919       0.79\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     35/49    0.667G   0.02081   0.01146   0.01236        38       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.02it/s]\n",
      "                 all        631        715      0.899      0.857      0.916      0.783\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     36/49    0.667G   0.02027   0.01124   0.01322        43       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.09it/s]\n",
      "                 all        631        715      0.901      0.866      0.927      0.791\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     37/49    0.667G   0.01995   0.01129   0.01135        49       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.07it/s]\n",
      "                 all        631        715      0.922      0.863      0.929      0.794\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     38/49    0.667G   0.01987   0.01138    0.0107        51       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.08it/s]\n",
      "                 all        631        715      0.892      0.877      0.926      0.801\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     39/49    0.667G   0.01961   0.01094   0.01044        50       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.04it/s]\n",
      "                 all        631        715      0.916      0.873      0.927      0.797\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     40/49    0.667G   0.01909    0.0107   0.01202        38       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.09it/s]\n",
      "                 all        631        715      0.916      0.857      0.924      0.789\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     41/49    0.667G   0.01888   0.01083   0.01183        37       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.09it/s]\n",
      "                 all        631        715      0.931      0.861      0.929        0.8\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     42/49    0.667G   0.01909   0.01081   0.01076        40       256: 100% 158/158 [00:44<00:00,  3.51it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.01it/s]\n",
      "                 all        631        715      0.896       0.89      0.932      0.813\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     43/49    0.667G   0.01886   0.01074   0.00954        52       256: 100% 158/158 [00:44<00:00,  3.53it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.04it/s]\n",
      "                 all        631        715      0.914      0.859       0.93      0.803\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     44/49    0.667G   0.01837   0.01074  0.008897        48       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.08it/s]\n",
      "                 all        631        715      0.922      0.879      0.935      0.803\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     45/49    0.667G   0.01822   0.01069  0.008984        41       256: 100% 158/158 [00:44<00:00,  3.52it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:05<00:00,  3.97it/s]\n",
      "                 all        631        715      0.916      0.882      0.938      0.817\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     46/49    0.667G   0.01741   0.01048  0.008611        50       256: 100% 158/158 [00:45<00:00,  3.51it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.03it/s]\n",
      "                 all        631        715      0.901      0.894      0.941      0.821\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     47/49    0.667G   0.01761   0.01052  0.008714        35       256: 100% 158/158 [00:45<00:00,  3.51it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.08it/s]\n",
      "                 all        631        715       0.89      0.898      0.938      0.817\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     48/49    0.667G   0.01767   0.01052  0.008873        43       256: 100% 158/158 [00:45<00:00,  3.51it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.04it/s]\n",
      "                 all        631        715      0.924      0.885      0.942      0.815\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     49/49    0.667G   0.01737    0.0103   0.00852        44       256: 100% 158/158 [00:45<00:00,  3.51it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:04<00:00,  4.06it/s]\n",
      "                 all        631        715       0.92       0.88       0.94      0.816\n",
      "\n",
      "50 epochs completed in 0.702 hours.\n",
      "Optimizer stripped from runs/train/exp/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from runs/train/exp/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating runs/train/exp/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7039792 parameters, 0 gradients, 15.9 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 20/20 [00:06<00:00,  2.91it/s]\n",
      "                 all        631        715      0.901      0.894      0.941      0.821\n",
      "    Corn_Common_Rust        631         58      0.945      0.885      0.937      0.543\n",
      "        Corn_Healthy        631         62      0.992      0.919      0.954      0.888\n",
      "Corn_Northern_Leaf_Blight        631         75      0.848       0.84      0.896      0.712\n",
      " Potato_Early_Blight        631         65      0.896      0.877      0.945      0.873\n",
      "      Potato_Healthy        631         31       0.89          1      0.988      0.957\n",
      "  Potato_Late_Blight        631         60        0.9        0.9      0.927      0.847\n",
      "Tomato_Bacterial_Spot        631         63      0.999          1      0.995      0.971\n",
      " Tomato_Early_Blight        631         79      0.917      0.696       0.84      0.655\n",
      "      Tomato_Healthy        631         78      0.872      0.875      0.957      0.874\n",
      "  Tomato_Late_Blight        631         95      0.828      0.842      0.916       0.74\n",
      " Tomato_Mosiac_Virus        631         49      0.823          1      0.995      0.977\n",
      "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5s on COCO128 for 3 epochs\n",
    "!python train.py --img 256 --batch 16 --epochs 50 --data dataset.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15glLzbQx5u0"
   },
   "source": [
    "# 4. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLI1JmHU7B0l"
   },
   "source": [
    "## Weights & Biases Logging ðŸŒŸ NEW\n",
    "\n",
    "[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
    "\n",
    "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WPvRbS5Swl6"
   },
   "source": [
    "## Local Logging\n",
    "\n",
    "All results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and val jpgs to see mosaics, labels, predictions and augmentation effects. Note an Ultralytics **Mosaic Dataloader** is used for training (shown below), which combines 4 images into 1 mosaic during training.\n",
    "\n",
    "Training results are automatically logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) as `results.csv`, which is plotted as `results.png` (below) after training completes. You can also plot any `results.csv` file manually:\n",
    "\n",
    "```python\n",
    "from utils.plots import plot_results \n",
    "plot_results('path/to/results.csv')  # plot 'results.csv' as 'results.png'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2k9Yi7MPNoy"
   },
   "outputs": [],
   "source": [
    "from utils.plots import plot_results \n",
    "plot_results('/content/Train-3/results.png')  # plot 'results.csv' as 'results.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zelyeqbyt3GD"
   },
   "source": [
    "# Environments\n",
    "\n",
    "YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n",
    "\n",
    "- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
    "- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n",
    "- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n",
    "- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qu7Iesl0p54"
   },
   "source": [
    "# Status\n",
    "\n",
    "![CI CPU testing](https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg)\n",
    "\n",
    "If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEijrePND_2I"
   },
   "source": [
    "# Appendix\n",
    "\n",
    "Optional extras below. Unit tests validate repo functionality and should be run on any PRs submitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcKoSIK2WSzj"
   },
   "outputs": [],
   "source": [
    "# Reproduce\n",
    "for x in 'yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x':\n",
    "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --task speed  # speed\n",
    "  !python val.py --weights {x}.pt --data coco.yaml --img 640 --conf 0.001 --iou 0.65  # mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMusP4OAxFu6"
   },
   "outputs": [],
   "source": [
    "# PyTorch Hub\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Images\n",
    "dir = 'https://ultralytics.com/images/'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGH0ZjkGjejy"
   },
   "outputs": [],
   "source": [
    "# CI Checks\n",
    "%%shell\n",
    "export PYTHONPATH=\"$PWD\"  # to run *.py. files in subdirectories\n",
    "rm -rf runs  # remove runs/\n",
    "for m in yolov5n; do  # models\n",
    "  python train.py --img 64 --batch 32 --weights $m.pt --epochs 1 --device 0  # train pretrained\n",
    "  python train.py --img 64 --batch 32 --weights '' --cfg $m.yaml --epochs 1 --device 0  # train scratch\n",
    "  for d in 0 cpu; do  # devices\n",
    "    python val.py --weights $m.pt --device $d # val official\n",
    "    python val.py --weights runs/train/exp/weights/best.pt --device $d # val custom\n",
    "    python detect.py --weights $m.pt --device $d  # detect official\n",
    "    python detect.py --weights runs/train/exp/weights/best.pt --device $d  # detect custom\n",
    "  done\n",
    "  python hubconf.py  # hub\n",
    "  python models/yolo.py --cfg $m.yaml  # build PyTorch model\n",
    "  python models/tf.py --weights $m.pt  # build TensorFlow model\n",
    "  python export.py --img 64 --batch 1 --weights $m.pt --include torchscript onnx  # export\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gogI-kwi3Tye"
   },
   "outputs": [],
   "source": [
    "# Profile\n",
    "from utils.torch_utils import profile\n",
    "\n",
    "m1 = lambda x: x * torch.sigmoid(x)\n",
    "m2 = torch.nn.SiLU()\n",
    "results = profile(input=torch.randn(16, 3, 640, 640), ops=[m1, m2], n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVRSOhEvUdb5"
   },
   "outputs": [],
   "source": [
    "# Evolve\n",
    "!python train.py --img 640 --batch 64 --epochs 100 --data coco128.yaml --weights yolov5s.pt --cache --noautoanchor --evolve\n",
    "!d=runs/train/evolve && cp evolve.* $d && zip -r evolve.zip $d && gsutil mv evolve.zip gs://bucket  # upload results (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSgFCAcMbk1R"
   },
   "outputs": [],
   "source": [
    "# VOC\n",
    "for b, m in zip([64, 64, 32, 16], ['yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']):  # zip(batch_size, model)\n",
    "  !python train.py --batch {b} --weights {m}.pt --data VOC.yaml --epochs 50 --cache --img 512 --nosave --hyp hyp.VOC.yaml --project VOC --name {m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTRwsvA9u7ln"
   },
   "outputs": [],
   "source": [
    "# TensorRT \n",
    "# https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-pip\n",
    "!pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # install\n",
    "!python export.py --weights yolov5s.pt --include engine --imgsz 640 640 --device 0  # export\n",
    "!python detect.py --weights yolov5s.engine --imgsz 640 640 --device 0  # inference"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "IEijrePND_2I"
   ],
   "name": "YOLOv5 Train-3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ce7164fc0c74bb9a2b5c7037375a727": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25621cff5d16448cb7260e839fd0f543": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "384a001876054c93b0af45cd1e960bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25621cff5d16448cb7260e839fd0f543",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9f09facb2a6c4a7096810d327c8b551c",
      "value": "100%"
     }
    },
    "473371611126476c88d5d42ec7031ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5296d28be75740b2892ae421bbec3657": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65efdfd0d26c46e79c8c5ff3b77126cc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_473371611126476c88d5d42ec7031ed6",
      "value": " 780M/780M [00:11&lt;00:00, 91.9MB/s]"
     }
    },
    "65efdfd0d26c46e79c8c5ff3b77126cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "769ecde6f2e64bacb596ce972f8d3d2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f09facb2a6c4a7096810d327c8b551c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4c4593c10904cb5b8a5724d60c7e181": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dded0aeae74440f7ba2ffa0beb8dd612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4c4593c10904cb5b8a5724d60c7e181",
      "max": 818322941,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ce7164fc0c74bb9a2b5c7037375a727",
      "value": 818322941
     }
    },
    "eb95db7cae194218b3fcefb439b6352f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_384a001876054c93b0af45cd1e960bfe",
       "IPY_MODEL_dded0aeae74440f7ba2ffa0beb8dd612",
       "IPY_MODEL_5296d28be75740b2892ae421bbec3657"
      ],
      "layout": "IPY_MODEL_769ecde6f2e64bacb596ce972f8d3d2d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
